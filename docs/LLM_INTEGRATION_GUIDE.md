# LLM æ¨¡å‹é›†æˆæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [æŠ€æœ¯é€‰å‹åˆ†æ](#æŠ€æœ¯é€‰å‹åˆ†æ)
2. [æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ](#æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ)
3. [é›†æˆå®ç°æµç¨‹](#é›†æˆå®ç°æµç¨‹)
4. [ä¼˜åŒ–ç­–ç•¥](#ä¼˜åŒ–ç­–ç•¥)
5. [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)

---

## 1. æŠ€æœ¯é€‰å‹åˆ†æ

### 1.1 å¯é€‰ LLM æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | ä¸­æ–‡æ”¯æŒ | æ¨èåº¦ |
|------|--------|----------|----------|--------|
| LLaMA-2-7B | 7B | ~14GB | â­â­ | â­â­â­ |
| Mistral-7B | 7B | ~14GB | â­â­â­ | â­â­â­â­ |
| ChatGLM3-6B | 6B | ~12GB | â­â­â­â­â­ | â­â­â­â­â­ |
| Qwen-7B | 7B | ~14GB | â­â­â­â­â­ | â­â­â­â­â­ |
| Baichuan2-7B | 7B | ~14GB | â­â­â­â­â­ | â­â­â­â­ |

### 1.2 æ¨èæ–¹æ¡ˆ

**é¦–é€‰: ChatGLM3-6B**
- âœ… ä¸­æ–‡æ”¯æŒä¼˜ç§€
- âœ… æ˜¾å­˜éœ€æ±‚é€‚ä¸­ï¼ˆ12GBï¼‰
- âœ… å¼€æºå…è´¹
- âœ… ç¤¾åŒºæ´»è·ƒ
- âœ… æ–‡æ¡£å®Œå–„

**å¤‡é€‰: Qwen-7B**
- âœ… é˜¿é‡Œå¼€æºï¼Œè´¨é‡é«˜
- âœ… ä¸­æ–‡èƒ½åŠ›å¼º
- âœ… æ”¯æŒé•¿æ–‡æœ¬

---

## 2. æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ

### 2.1 æ–¹æ¡ˆä¸€ï¼šHugging Face Hubï¼ˆæ¨èï¼‰

#### ä¼˜ç‚¹
- å®˜æ–¹æ¸ é“ï¼Œå®‰å…¨å¯é 
- è‡ªåŠ¨ç®¡ç†ç¼“å­˜
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 

#### ä¸‹è½½æ­¥éª¤

```bash
# 1. å®‰è£…ä¾èµ–
pip install transformers torch accelerate

# 2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼ŒåŠ é€Ÿä¸‹è½½ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# 3. Python ä»£ç ä¸‹è½½
from transformers import AutoModel, AutoTokenizer

model_name = "THUDM/chatglm3-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° ~/.cache/huggingface/hub/
```

#### æ‰‹åŠ¨ä¸‹è½½ï¼ˆå›½å†…é•œåƒï¼‰

```bash
# ä½¿ç”¨ huggingface-cli
pip install huggingface_hub

# ä¸‹è½½æ¨¡å‹
huggingface-cli download THUDM/chatglm3-6b \
  --local-dir ./models/chatglm3-6b \
  --local-dir-use-symlinks False
```

### 2.2 æ–¹æ¡ˆäºŒï¼šModelScopeï¼ˆå›½å†…æ¨èï¼‰

```bash
# 1. å®‰è£… ModelScope
pip install modelscope

# 2. ä¸‹è½½æ¨¡å‹
from modelscope import snapshot_download

model_dir = snapshot_download(
    'ZhipuAI/chatglm3-6b',
    cache_dir='./models'
)
```

### 2.3 æ–¹æ¡ˆä¸‰ï¼šGit LFSï¼ˆå®Œæ•´ä¸‹è½½ï¼‰

```bash
# 1. å®‰è£… Git LFS
git lfs install

# 2. å…‹éš†ä»“åº“
git clone https://huggingface.co/THUDM/chatglm3-6b ./models/chatglm3-6b

# æˆ–ä½¿ç”¨é•œåƒ
git clone https://hf-mirror.com/THUDM/chatglm3-6b ./models/chatglm3-6b
```

---

## 3. é›†æˆå®ç°æµç¨‹

### 3.1 é¡¹ç›®ç»“æ„è°ƒæ•´

```
backend/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ chatglm3-6b/          # æ¨¡å‹æ–‡ä»¶ç›®å½•
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm_service.py        # ä¿®æ”¹æ­¤æ–‡ä»¶
â”‚   â””â”€â”€ model_loader.py       # ä¿®æ”¹æ­¤æ–‡ä»¶
â””â”€â”€ config.py                 # æ·»åŠ æ¨¡å‹é…ç½®
```

### 3.2 é…ç½®æ–‡ä»¶ä¿®æ”¹

**backend/config.py**

```python
# LLM æ¨¡å‹é…ç½®
LLM_CONFIG = {
    "model_name": "THUDM/chatglm3-6b",
    "model_path": "./models/chatglm3-6b",  # æœ¬åœ°è·¯å¾„
    "device": "cuda",  # cuda æˆ– cpu
    "use_fp16": True,  # ä½¿ç”¨åŠç²¾åº¦
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
}
```

### 3.3 æ¨¡å‹åŠ è½½å™¨å®ç°

**backend/services/model_loader.py**

```python
import torch
from transformers import AutoModel, AutoTokenizer
from utils.logger import setup_logger
from config import LLM_CONFIG

logger = setup_logger(__name__)

class LLMModelLoader:
    """LLM æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = LLM_CONFIG["device"]
        
    def load_model(self):
        """åŠ è½½ ChatGLM3 æ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {LLM_CONFIG['model_name']}")
            
            # åŠ è½½ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                LLM_CONFIG["model_path"],
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                LLM_CONFIG["model_path"],
                trust_remote_code=True
            )
            
            # ç§»åŠ¨åˆ° GPU å¹¶ä½¿ç”¨åŠç²¾åº¦
            if self.device == "cuda" and torch.cuda.is_available():
                self.model = self.model.cuda()
                if LLM_CONFIG["use_fp16"]:
                    self.model = self.model.half()
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        # åˆå¹¶é…ç½®
        gen_kwargs = {
            "max_length": LLM_CONFIG["max_length"],
            "temperature": LLM_CONFIG["temperature"],
            "top_p": LLM_CONFIG["top_p"],
            "do_sample": LLM_CONFIG["do_sample"],
        }
        gen_kwargs.update(kwargs)
        
        # ç”Ÿæˆ
        response, history = self.model.chat(
            self.tokenizer,
            prompt,
            history=[],
            **gen_kwargs
        )
        
        return response
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹"""
        if self.model:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("æ¨¡å‹å·²å¸è½½")

# å…¨å±€å®ä¾‹
llm_loader = LLMModelLoader()
```

### 3.4 LLM æœåŠ¡å®ç°

**backend/services/llm_service.py**

```python
import json
import re
from typing import Dict, List
from services.model_loader import llm_loader
from utils.logger import setup_logger

logger = setup_logger(__name__)

# æç¤ºè¯æ¨¡æ¿
SCRIPT_GENERATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘è„šæœ¬åˆ›ä½œåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„åˆ›ä½œæŒ‡ä»¤ï¼Œç”Ÿæˆè¯¦ç»†çš„è§†é¢‘è„šæœ¬å’Œåˆ†é•œã€‚

ç”¨æˆ·æŒ‡ä»¤ï¼š{user_prompt}

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "title": "è§†é¢‘æ ‡é¢˜",
  "total_duration": æ€»æ—¶é•¿ï¼ˆç§’ï¼‰,
  "scenes": [
    {{
      "scene_number": 1,
      "description": "åœºæ™¯æè¿°ï¼ˆè¯¦ç»†çš„è§†è§‰æè¿°ï¼‰",
      "duration": 5,
      "camera": "é•œå¤´ç±»å‹ï¼ˆwide shot/close up/medium shotï¼‰",
      "action": "åŠ¨ä½œæè¿°"
    }}
  ]
}}

è¦æ±‚ï¼š
1. æ¯ä¸ªåœºæ™¯æè¿°è¦å…·ä½“ã€ç”ŸåŠ¨
2. åœºæ™¯ä¹‹é—´è¦æœ‰è¿è´¯æ€§
3. æ¯ä¸ªåœºæ™¯æ—¶é•¿ 3-8 ç§’
4. è‡³å°‘ç”Ÿæˆ 3 ä¸ªåœºæ™¯
5. åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹
"""

def generate_script(prompt: str) -> Dict:
    """
    ä½¿ç”¨ LLM ç”Ÿæˆè§†é¢‘è„šæœ¬
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„åˆ›ä½œæŒ‡ä»¤
        
    Returns:
        åŒ…å«åˆ†é•œä¿¡æ¯çš„å­—å…¸
    """
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆè„šæœ¬ï¼Œç”¨æˆ·è¾“å…¥: {prompt}")
        
        # æ„é€ å®Œæ•´æç¤ºè¯
        full_prompt = SCRIPT_GENERATION_PROMPT.format(user_prompt=prompt)
        
        # è°ƒç”¨ LLM ç”Ÿæˆ
        response = llm_loader.generate(
            full_prompt,
            max_length=2048,
            temperature=0.7
        )
        
        logger.info(f"LLM åŸå§‹è¾“å‡º: {response}")
        
        # è§£æ JSON
        script = parse_llm_response(response)
        
        # éªŒè¯å’Œä¿®æ­£
        script = validate_and_fix_script(script)
        
        logger.info(f"è„šæœ¬ç”ŸæˆæˆåŠŸï¼Œå…± {len(script['scenes'])} ä¸ªåœºæ™¯")
        return script
        
    except Exception as e:
        logger.error(f"è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
        # è¿”å›å¤‡ç”¨è„šæœ¬
        return generate_fallback_script(prompt)

def parse_llm_response(response: str) -> Dict:
    """è§£æ LLM è¾“å‡ºçš„ JSON"""
    try:
        # æå– JSON éƒ¨åˆ†
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            return json.loads(json_str)
        else:
            raise ValueError("æœªæ‰¾åˆ° JSON æ ¼å¼")
    except Exception as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {str(e)}")
        raise

def validate_and_fix_script(script: Dict) -> Dict:
    """éªŒè¯å’Œä¿®æ­£è„šæœ¬æ ¼å¼"""
    # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
    if "title" not in script:
        script["title"] = "è‡ªåŠ¨ç”Ÿæˆè§†é¢‘"
    
    if "scenes" not in script or not script["scenes"]:
        raise ValueError("è„šæœ¬ä¸­æ²¡æœ‰åœºæ™¯")
    
    # ä¿®æ­£åœºæ™¯ç¼–å·
    for i, scene in enumerate(script["scenes"]):
        scene["scene_number"] = i + 1
        
        # ç¡®ä¿å¿…è¦å­—æ®µ
        if "description" not in scene:
            scene["description"] = f"åœºæ™¯ {i+1}"
        if "duration" not in scene:
            scene["duration"] = 5
        if "camera" not in scene:
            scene["camera"] = "wide shot"
    
    # è®¡ç®—æ€»æ—¶é•¿
    script["total_duration"] = sum(s["duration"] for s in script["scenes"])
    
    return script

def generate_fallback_script(prompt: str) -> Dict:
    """ç”Ÿæˆå¤‡ç”¨è„šæœ¬ï¼ˆå½“ LLM å¤±è´¥æ—¶ï¼‰"""
    logger.warning("ä½¿ç”¨å¤‡ç”¨è„šæœ¬ç”Ÿæˆ")
    
    # ç®€å•åˆ†å¥
    sentences = re.split(r'[ï¼Œã€‚,.]', prompt)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    scenes = []
    for i, sentence in enumerate(sentences[:5]):  # æœ€å¤š5ä¸ªåœºæ™¯
        scenes.append({
            "scene_number": i + 1,
            "description": sentence,
            "duration": 5,
            "camera": "wide shot",
            "action": "å±•ç¤ºåœºæ™¯"
        })
    
    return {
        "title": "è‡ªåŠ¨ç”Ÿæˆè§†é¢‘",
        "total_duration": len(scenes) * 5,
        "scenes": scenes
    }

def optimize_prompt_for_video(scene_description: str) -> str:
    """
    ä¼˜åŒ–åœºæ™¯æè¿°ä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ Prompt
    
    Args:
        scene_description: åœºæ™¯æè¿°
        
    Returns:
        ä¼˜åŒ–åçš„ Prompt
    """
    # æ·»åŠ è§†è§‰è´¨é‡å…³é”®è¯
    quality_keywords = "high quality, cinematic, detailed, 4k, professional"
    
    # æ„é€ å®Œæ•´ Prompt
    prompt = f"{scene_description}, {quality_keywords}"
    
    return prompt
```

### 3.5 å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹

**backend/main.py**

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from services.model_loader import llm_loader
from utils.logger import setup_logger

logger = setup_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    logger.info("åº”ç”¨å¯åŠ¨ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹...")
    success = llm_loader.load_model()
    if not success:
        logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    
    yield
    
    # å…³é—­æ—¶å¸è½½æ¨¡å‹
    logger.info("åº”ç”¨å…³é—­ï¼Œå¸è½½æ¨¡å‹...")
    llm_loader.unload_model()

app = FastAPI(
    title="å¤šæ¨¡æ€è§†é¢‘åˆ›ä½œå¹³å°",
    lifespan=lifespan
)
```

---

## 4. ä¼˜åŒ–ç­–ç•¥

### 4.1 æ˜¾å­˜ä¼˜åŒ–

#### æ–¹æ¡ˆä¸€ï¼šåŠç²¾åº¦ï¼ˆFP16ï¼‰
```python
model = model.half()  # æ˜¾å­˜å‡åŠ
```

#### æ–¹æ¡ˆäºŒï¼šINT8 é‡åŒ–
```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModel.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    device_map="auto"
)
```

#### æ–¹æ¡ˆä¸‰ï¼šINT4 é‡åŒ–ï¼ˆæœ€æ¿€è¿›ï¼‰
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### 4.2 æ¨ç†åŠ é€Ÿ

#### ä½¿ç”¨ Flash Attention
```bash
pip install flash-attn
```

```python
model = AutoModel.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2"
)
```

#### æ‰¹é‡æ¨ç†
```python
# æ‰¹é‡ç”Ÿæˆå¤šä¸ªåœºæ™¯çš„æè¿°
responses = model.batch_generate(prompts)
```

### 4.3 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def generate_script_cached(prompt: str) -> Dict:
    """å¸¦ç¼“å­˜çš„è„šæœ¬ç”Ÿæˆ"""
    return generate_script(prompt)
```

---

## 5. æµ‹è¯•éªŒè¯

### 5.1 å•å…ƒæµ‹è¯•

**tests/test_llm_service.py**

```python
import pytest
from services.llm_service import generate_script, parse_llm_response

def test_generate_script():
    """æµ‹è¯•è„šæœ¬ç”Ÿæˆ"""
    prompt = "åˆ¶ä½œä¸€æ®µå…³äºæ£®æ—æ¢é™©çš„çŸ­è§†é¢‘"
    script = generate_script(prompt)
    
    assert "scenes" in script
    assert len(script["scenes"]) > 0
    assert script["scenes"][0]["scene_number"] == 1

def test_parse_llm_response():
    """æµ‹è¯• JSON è§£æ"""
    response = '''
    {
      "title": "æµ‹è¯•è§†é¢‘",
      "scenes": [
        {"scene_number": 1, "description": "åœºæ™¯1", "duration": 5}
      ]
    }
    '''
    script = parse_llm_response(response)
    assert script["title"] == "æµ‹è¯•è§†é¢‘"
```

### 5.2 é›†æˆæµ‹è¯•

```python
def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    from services.model_loader import llm_loader
    
    # åŠ è½½æ¨¡å‹
    llm_loader.load_model()
    
    # ç”Ÿæˆè„šæœ¬
    script = generate_script("åˆ¶ä½œä¸€æ®µå…³äºæµ·æ»©æ—¥è½çš„è§†é¢‘")
    
    # éªŒè¯ç»“æœ
    assert len(script["scenes"]) >= 3
    
    # å¸è½½æ¨¡å‹
    llm_loader.unload_model()
```

### 5.3 æ€§èƒ½æµ‹è¯•

```python
import time

def test_generation_speed():
    """æµ‹è¯•ç”Ÿæˆé€Ÿåº¦"""
    start = time.time()
    script = generate_script("æµ‹è¯•æç¤ºè¯")
    end = time.time()
    
    duration = end - start
    print(f"ç”Ÿæˆè€—æ—¶: {duration:.2f} ç§’")
    assert duration < 10  # åº”åœ¨10ç§’å†…å®Œæˆ
```

---

## 6. å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨ INT8 æˆ– INT4 é‡åŒ–ï¼Œæˆ–è€…ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ ChatGLM3-6Bï¼‰

### Q2: ä¸‹è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨å›½å†…é•œåƒï¼ˆModelScope æˆ– HF-Mirrorï¼‰

### Q3: æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸å¯¹æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨æ›´è¯¦ç»†çš„æç¤ºè¯ï¼Œæˆ–è€…æ·»åŠ åå¤„ç†é€»è¾‘

### Q4: å¦‚ä½•åˆ‡æ¢å…¶ä»–æ¨¡å‹ï¼Ÿ
A: ä¿®æ”¹ config.py ä¸­çš„ model_name å’Œ model_path

---

## 7. å®æ–½æ—¶é—´è¡¨

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ |
|------|------|----------|
| 1 | ä¸‹è½½æ¨¡å‹ | 1-2 å°æ—¶ |
| 2 | ä¿®æ”¹é…ç½®æ–‡ä»¶ | 10 åˆ†é’Ÿ |
| 3 | å®ç°æ¨¡å‹åŠ è½½å™¨ | 30 åˆ†é’Ÿ |
| 4 | å®ç° LLM æœåŠ¡ | 1 å°æ—¶ |
| 5 | é›†æˆåˆ°ä¸»ç¨‹åº | 20 åˆ†é’Ÿ |
| 6 | æµ‹è¯•éªŒè¯ | 30 åˆ†é’Ÿ |
| **æ€»è®¡** | | **3-4 å°æ—¶** |

---

## 8. å‚è€ƒèµ„æº

- ChatGLM3 å®˜æ–¹æ–‡æ¡£: https://github.com/THUDM/ChatGLM3
- Transformers æ–‡æ¡£: https://huggingface.co/docs/transformers
- ModelScope æ–‡æ¡£: https://modelscope.cn/docs
- é‡åŒ–æŠ€æœ¯: https://huggingface.co/docs/transformers/quantization
