# è§†é¢‘ç”Ÿæˆæ¨¡å‹é›†æˆæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [æŠ€æœ¯é€‰å‹åˆ†æ](#æŠ€æœ¯é€‰å‹åˆ†æ)
2. [æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ](#æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ)
3. [é›†æˆå®ç°æµç¨‹](#é›†æˆå®ç°æµç¨‹)
4. [ä¼˜åŒ–ç­–ç•¥](#ä¼˜åŒ–ç­–ç•¥)
5. [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)

---

## 1. æŠ€æœ¯é€‰å‹åˆ†æ

### 1.1 å¯é€‰è§†é¢‘ç”Ÿæˆæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | è´¨é‡ | æ¨èåº¦ |
|------|--------|----------|----------|------|--------|
| Stable Diffusion Video | 7B | ~16GB | ä¸­ | ä¼˜ç§€ | â­â­â­â­â­ |
| ModelScope T2V | 3B | ~8GB | å¿« | è‰¯å¥½ | â­â­â­â­ |
| Damo Video | 5B | ~12GB | ä¸­ | ä¼˜ç§€ | â­â­â­â­ |
| Open Sora | 7B | ~16GB | æ…¢ | ä¼˜ç§€ | â­â­â­ |
| AnimateDiff | 1B | ~4GB | å¿« | ä¸€èˆ¬ | â­â­â­ |

### 1.2 æ¨èæ–¹æ¡ˆ

**é¦–é€‰: Stable Diffusion Video (SVD)**
- âœ… è´¨é‡æœ€å¥½
- âœ… ç¤¾åŒºæ´»è·ƒ
- âœ… æ–‡æ¡£å®Œå–„
- âœ… æ˜“äºé›†æˆ
- âœ… æ”¯æŒå¤šç§è¾“å…¥

**å¤‡é€‰: ModelScope T2V**
- âœ… æ˜¾å­˜éœ€æ±‚ä½
- âœ… æ¨ç†é€Ÿåº¦å¿«
- âœ… ä¸­æ–‡æ”¯æŒå¥½
- âœ… å›½å†…éƒ¨ç½²å‹å¥½

---

## 2. æ¨¡å‹ä¸‹è½½æ–¹æ¡ˆ

### 2.1 Stable Diffusion Video ä¸‹è½½

#### æ–¹æ¡ˆä¸€ï¼šHugging Faceï¼ˆæ¨èï¼‰

```bash
# 1. å®‰è£…ä¾èµ–
pip install diffusers transformers torch accelerate

# 2. Python ä¸‹è½½
from diffusers import StableVideoDiffusionPipeline
import torch

model_id = "stabilityai/stable-video-diffusion-img2vid-xt"
pipe = StableVideoDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    variant="fp16"
)

# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° ~/.cache/huggingface/hub/
```

#### æ–¹æ¡ˆäºŒï¼šModelScopeï¼ˆå›½å†…æ¨èï¼‰

```bash
# 1. å®‰è£… ModelScope
pip install modelscope

# 2. Python ä¸‹è½½
from modelscope import snapshot_download

model_dir = snapshot_download(
    'damo/text-to-video-synthesis',
    cache_dir='./models'
)
```

#### æ–¹æ¡ˆä¸‰ï¼šGit LFS

```bash
# 1. å®‰è£… Git LFS
git lfs install

# 2. å…‹éš†ä»“åº“
git clone https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt \
  ./models/svd-xt
```

### 2.2 æ¨¡å‹æ–‡ä»¶ç»“æ„

```
backend/models/
â”œâ”€â”€ svd-xt/                          # Stable Diffusion Video
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin
â”‚   â”œâ”€â”€ model_index.json
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ text_encoder/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ unet/
â”‚   â””â”€â”€ vae/
â””â”€â”€ t2v-model/                       # ModelScope T2Vï¼ˆå¤‡é€‰ï¼‰
    â”œâ”€â”€ config.json
    â””â”€â”€ pytorch_model.bin
```

---

## 3. é›†æˆå®ç°æµç¨‹

### 3.1 é¡¹ç›®ç»“æ„è°ƒæ•´

```
backend/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ svd-xt/                      # è§†é¢‘ç”Ÿæˆæ¨¡å‹
â”‚   â””â”€â”€ chatglm3-6b/                 # LLM æ¨¡å‹
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm_service.py               # LLM æœåŠ¡ï¼ˆå·²å®Œæˆï¼‰
â”‚   â”œâ”€â”€ video_service.py             # è§†é¢‘ç”ŸæˆæœåŠ¡ï¼ˆå¾…æ›´æ–°ï¼‰
â”‚   â”œâ”€â”€ model_loader.py              # æ¨¡å‹åŠ è½½å™¨ï¼ˆå¾…æ›´æ–°ï¼‰
â”‚   â””â”€â”€ video_processor.py            # è§†é¢‘å¤„ç†å™¨ï¼ˆæ–°å¢ï¼‰
â””â”€â”€ config.py                        # é…ç½®æ–‡ä»¶ï¼ˆå¾…æ›´æ–°ï¼‰
```

### 3.2 é…ç½®æ–‡ä»¶ä¿®æ”¹

**backend/config.py**

```python
# è§†é¢‘ç”Ÿæˆæ¨¡å‹é…ç½®
VIDEO_CONFIG = {
    "model_name": "stabilityai/stable-video-diffusion-img2vid-xt",
    "model_path": "./models/svd-xt",  # æœ¬åœ°è·¯å¾„
    "device": "cuda",
    "use_fp16": True,
    "num_inference_steps": 25,
    "guidance_scale": 7.5,
    "height": 576,
    "width": 1024,
    "num_frames": 25,  # ç”Ÿæˆå¸§æ•°
    "fps": 6,  # å¸§ç‡
    "auto_download": True,
}

# è§†é¢‘å¤„ç†é…ç½®
VIDEO_PROCESSING_CONFIG = {
    "output_format": "mp4",
    "codec": "libx264",
    "bitrate": "5000k",
    "enable_interpolation": False,  # å¸§æ’å€¼
}
```

### 3.3 è§†é¢‘æ¨¡å‹åŠ è½½å™¨å®ç°

**backend/services/model_loader.pyï¼ˆæ›´æ–°ï¼‰**

```python
import torch
from diffusers import StableVideoDiffusionPipeline
from utils.logger import setup_logger
from config import VIDEO_CONFIG

logger = setup_logger(__name__)

class VideoModelLoader:
    """è§†é¢‘ç”Ÿæˆæ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self):
        self.model = None
        self.device = VIDEO_CONFIG["device"] if torch.cuda.is_available() else "cpu"
        self.is_loaded = False
        logger.info(f"è§†é¢‘æ¨¡å‹åŠ è½½å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_model(self):
        """åŠ è½½ Stable Diffusion Video æ¨¡å‹"""
        if self.is_loaded:
            logger.info("è§†é¢‘æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡")
            return True
        
        try:
            logger.info(f"å¼€å§‹åŠ è½½è§†é¢‘æ¨¡å‹: {VIDEO_CONFIG['model_name']}")
            
            model_path = VIDEO_CONFIG["model_path"]
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not os.path.exists(model_path) and VIDEO_CONFIG.get("auto_download", False):
                logger.info(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä» Hugging Face ä¸‹è½½...")
                model_path = VIDEO_CONFIG["model_name"]
            elif not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                return False
            
            # åŠ è½½æ¨¡å‹
            logger.info("åŠ è½½ Stable Diffusion Video æ¨¡å‹...")
            
            load_kwargs = {
                "torch_dtype": torch.float16 if VIDEO_CONFIG.get("use_fp16") else torch.float32,
            }
            
            if VIDEO_CONFIG.get("use_fp16"):
                load_kwargs["variant"] = "fp16"
            
            self.model = StableVideoDiffusionPipeline.from_pretrained(
                model_path,
                **load_kwargs
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device == "cuda":
                self.model = self.model.to(self.device)
                
                # å¯ç”¨å†…å­˜ä¼˜åŒ–
                self.model.enable_attention_slicing()
                
                # å¯ç”¨ xFormers åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    self.model.enable_xformers_memory_efficient_attention()
                    logger.info("å¯ç”¨ xFormers åŠ é€Ÿ")
                except:
                    logger.warning("xFormers ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›")
            
            self.is_loaded = True
            logger.info("âœ… è§†é¢‘æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                logger.info(f"GPU æ˜¾å­˜ä½¿ç”¨: {memory_allocated:.2f} GB")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def generate_video(self, prompt: str, image=None, **kwargs) -> list:
        """
        ç”Ÿæˆè§†é¢‘
        
        Args:
            prompt: æ–‡æœ¬æè¿°
            image: è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
            **kwargs: ç”Ÿæˆå‚æ•°
            
        Returns:
            è§†é¢‘å¸§åˆ—è¡¨
        """
        if not self.is_loaded or self.model is None:
            raise RuntimeError("è§†é¢‘æ¨¡å‹æœªåŠ è½½")
        
        try:
            # åˆå¹¶é…ç½®
            gen_kwargs = {
                "num_inference_steps": VIDEO_CONFIG.get("num_inference_steps", 25),
                "guidance_scale": VIDEO_CONFIG.get("guidance_scale", 7.5),
                "height": VIDEO_CONFIG.get("height", 576),
                "width": VIDEO_CONFIG.get("width", 1024),
                "num_frames": VIDEO_CONFIG.get("num_frames", 25),
            }
            gen_kwargs.update(kwargs)
            
            logger.info(f"ç”Ÿæˆè§†é¢‘ï¼Œå‚æ•°: {gen_kwargs}")
            
            # ç”Ÿæˆè§†é¢‘
            if image is not None:
                # å›¾åƒåˆ°è§†é¢‘
                output = self.model(
                    image=image,
                    prompt=prompt,
                    **gen_kwargs
                )
            else:
                # æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆéœ€è¦å…ˆç”Ÿæˆå›¾åƒï¼‰
                logger.warning("éœ€è¦è¾“å…¥å›¾åƒï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ")
                from PIL import Image
                import numpy as np
                
                # åˆ›å»ºé»˜è®¤å›¾åƒ
                image = Image.new('RGB', (gen_kwargs["width"], gen_kwargs["height"]))
                output = self.model(
                    image=image,
                    prompt=prompt,
                    **gen_kwargs
                )
            
            frames = output.frames[0]  # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„å¸§
            return frames
            
        except Exception as e:
            logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
        if self.model:
            del self.model
            self.model = None
            self.is_loaded = False
            
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("è§†é¢‘æ¨¡å‹å·²å¸è½½")

# å…¨å±€å®ä¾‹
video_loader = VideoModelLoader()
```

### 3.4 è§†é¢‘å¤„ç†å™¨å®ç°

**backend/services/video_processor.pyï¼ˆæ–°å¢ï¼‰**

```python
"""
è§†é¢‘å¤„ç†æ¨¡å— - è´Ÿè´£è§†é¢‘å¸§å¤„ç†å’Œç¼–ç 
"""
import cv2
import numpy as np
from typing import List
from PIL import Image
import os
from utils.logger import setup_logger
from config import VIDEO_PROCESSING_CONFIG, VIDEO_OUTPUT_DIR

logger = setup_logger(__name__)

class VideoProcessor:
    """è§†é¢‘å¤„ç†å™¨"""
    
    @staticmethod
    def frames_to_video(frames: List, output_path: str, fps: int = 6) -> str:
        """
        å°†å¸§åˆ—è¡¨è½¬æ¢ä¸ºè§†é¢‘æ–‡ä»¶
        
        Args:
            frames: PIL Image åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
            fps: å¸§ç‡
            
        Returns:
            è§†é¢‘æ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info(f"å¼€å§‹è½¬æ¢è§†é¢‘ï¼Œå¸§æ•°: {len(frames)}, FPS: {fps}")
            
            # è½¬æ¢ä¸º numpy æ•°ç»„
            frame_array = []
            for frame in frames:
                if isinstance(frame, Image.Image):
                    frame = np.array(frame)
                
                # è½¬æ¢ä¸º BGRï¼ˆOpenCV æ ¼å¼ï¼‰
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                
                frame_array.append(frame)
            
            # è·å–è§†é¢‘å‚æ•°
            height, width = frame_array[0].shape[:2]
            
            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            # å†™å…¥å¸§
            for frame in frame_array:
                out.write(frame)
            
            out.release()
            
            logger.info(f"âœ… è§†é¢‘è½¬æ¢å®Œæˆ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"è§†é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
            raise
    
    @staticmethod
    def generate_placeholder_image(width: int = 1024, height: int = 576) -> Image.Image:
        """ç”Ÿæˆå ä½ç¬¦å›¾åƒ"""
        return Image.new('RGB', (width, height), color=(73, 109, 137))
    
    @staticmethod
    def interpolate_frames(frames: List, factor: int = 2) -> List:
        """
        å¸§æ’å€¼ï¼ˆå¢åŠ å¸§æ•°ï¼‰
        
        Args:
            frames: åŸå§‹å¸§åˆ—è¡¨
            factor: æ’å€¼å› å­
            
        Returns:
            æ’å€¼åçš„å¸§åˆ—è¡¨
        """
        logger.info(f"æ‰§è¡Œå¸§æ’å€¼ï¼Œå› å­: {factor}")
        
        interpolated = []
        for i in range(len(frames) - 1):
            interpolated.append(frames[i])
            
            # ç®€å•çš„çº¿æ€§æ’å€¼
            for j in range(1, factor):
                alpha = j / factor
                blended = Image.blend(frames[i], frames[i + 1], alpha)
                interpolated.append(blended)
        
        interpolated.append(frames[-1])
        return interpolated
```

### 3.5 è§†é¢‘ç”ŸæˆæœåŠ¡æ›´æ–°

**backend/services/video_service.pyï¼ˆæ›´æ–°ï¼‰**

```python
"""
è§†é¢‘ç”ŸæˆæœåŠ¡æ¨¡å— - è´Ÿè´£è§†é¢‘ç”Ÿæˆå’Œåå¤„ç†
"""
import os
from typing import List, Dict
from PIL import Image
from utils.logger import setup_logger
from config import VIDEO_OUTPUT_DIR, VIDEO_CONFIG
from services.video_processor import VideoProcessor

logger = setup_logger(__name__)

def generate_video_from_script(script: Dict, task_id: str) -> str:
    """
    æ ¹æ®è„šæœ¬ç”Ÿæˆå®Œæ•´è§†é¢‘
    
    Args:
        script: åŒ…å«åˆ†é•œä¿¡æ¯çš„è„šæœ¬å­—å…¸
        task_id: ä»»åŠ¡ID
        
    Returns:
        ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
    """
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆè§†é¢‘ï¼Œä»»åŠ¡ID: {task_id}")
        
        video_paths = []
        
        # ä¸ºæ¯ä¸ªåˆ†é•œç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        for scene in script['scenes']:
            logger.info(f"ç”Ÿæˆåœºæ™¯ {scene['scene_number']}: {scene['description']}")
            video_path = generate_scene_video(scene, task_id)
            video_paths.append(video_path)
        
        # æ‹¼æ¥æ‰€æœ‰è§†é¢‘ç‰‡æ®µ
        final_video_path = stitch_videos(video_paths, task_id)
        
        logger.info(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {final_video_path}")
        return final_video_path
        
    except Exception as e:
        logger.error(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise

def generate_scene_video(scene: Dict, task_id: str) -> str:
    """
    ç”Ÿæˆå•ä¸ªåœºæ™¯çš„è§†é¢‘ç‰‡æ®µ
    
    Args:
        scene: åœºæ™¯ä¿¡æ¯å­—å…¸
        task_id: ä»»åŠ¡ID
        
    Returns:
        è§†é¢‘ç‰‡æ®µæ–‡ä»¶è·¯å¾„
    """
    try:
        from services.model_loader import video_loader
        
        scene_id = scene['scene_number']
        output_path = os.path.join(VIDEO_OUTPUT_DIR, f"{task_id}_scene_{scene_id}.mp4")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        if not video_loader.is_loaded:
            logger.warning("è§†é¢‘æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            return generate_scene_video_fallback(scene, task_id)
        
        # ä¼˜åŒ–æç¤ºè¯
        from services.llm_service import optimize_prompt_for_video
        prompt = optimize_prompt_for_video(scene['description'])
        
        logger.info(f"ç”Ÿæˆæç¤ºè¯: {prompt}")
        
        # ç”Ÿæˆå ä½ç¬¦å›¾åƒ
        image = VideoProcessor.generate_placeholder_image(
            width=VIDEO_CONFIG.get("width", 1024),
            height=VIDEO_CONFIG.get("height", 576)
        )
        
        # ç”Ÿæˆè§†é¢‘å¸§
        frames = video_loader.generate_video(
            prompt=prompt,
            image=image,
            num_frames=VIDEO_CONFIG.get("num_frames", 25)
        )
        
        # å¸§æ’å€¼ï¼ˆå¯é€‰ï¼‰
        if VIDEO_CONFIG.get("enable_interpolation", False):
            frames = VideoProcessor.interpolate_frames(frames, factor=2)
        
        # è½¬æ¢ä¸ºè§†é¢‘æ–‡ä»¶
        fps = VIDEO_CONFIG.get("fps", 6)
        VideoProcessor.frames_to_video(frames, output_path, fps=fps)
        
        return output_path
        
    except Exception as e:
        logger.error(f"åœºæ™¯è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        return generate_scene_video_fallback(scene, task_id)

def generate_scene_video_fallback(scene: Dict, task_id: str) -> str:
    """
    ç”Ÿæˆåœºæ™¯è§†é¢‘çš„å¤‡ç”¨æ–¹æ¡ˆï¼ˆå½“æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼‰
    """
    logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆç”Ÿæˆåœºæ™¯ {scene['scene_number']}")
    
    scene_id = scene['scene_number']
    duration = scene['duration']
    fps = VIDEO_CONFIG.get("fps", 6)
    width = VIDEO_CONFIG.get("width", 1024)
    height = VIDEO_CONFIG.get("height", 576)
    
    output_path = os.path.join(VIDEO_OUTPUT_DIR, f"{task_id}_scene_{scene_id}.mp4")
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # ç”Ÿæˆéšæœºé¢œè‰²çš„å¸§
    import numpy as np
    color = np.random.randint(0, 255, 3).tolist()
    total_frames = duration * fps
    
    for _ in range(total_frames):
        frame = np.full((height, width, 3), color, dtype=np.uint8)
        
        # æ·»åŠ åœºæ™¯æè¿°æ–‡å­—
        import cv2
        text = f"Scene {scene_id}: {scene['description'][:40]}"
        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.7, (255, 255, 255), 2, cv2.LINE_AA)
        
        out.write(frame)
    
    out.release()
    return output_path

def stitch_videos(video_paths: List[str], task_id: str) -> str:
    """
    æ‹¼æ¥å¤šä¸ªè§†é¢‘ç‰‡æ®µ
    
    Args:
        video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        task_id: ä»»åŠ¡ID
        
    Returns:
        æ‹¼æ¥åçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
    """
    if not video_paths:
        raise ValueError("æ²¡æœ‰è§†é¢‘ç‰‡æ®µå¯æ‹¼æ¥")
    
    import cv2
    
    output_path = os.path.join(VIDEO_OUTPUT_DIR, f"{task_id}_final.mp4")
    
    logger.info(f"å¼€å§‹æ‹¼æ¥è§†é¢‘ï¼Œç‰‡æ®µæ•°: {len(video_paths)}")
    
    # è¯»å–ç¬¬ä¸€ä¸ªè§†é¢‘è·å–å‚æ•°
    cap = cv2.VideoCapture(video_paths[0])
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()
    
    # åˆ›å»ºè¾“å‡ºè§†é¢‘
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # é€ä¸ªè¯»å–å¹¶å†™å…¥è§†é¢‘ç‰‡æ®µ
    for video_path in video_paths:
        cap = cv2.VideoCapture(video_path)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            out.write(frame)
        cap.release()
    
    out.release()
    
    logger.info(f"âœ… è§†é¢‘æ‹¼æ¥å®Œæˆ: {output_path}")
    return output_path
```

### 3.6 ä¸»ç¨‹åºé›†æˆ

**backend/main.pyï¼ˆæ›´æ–°ï¼‰**

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("=" * 60)
    logger.info("åº”ç”¨å¯åŠ¨ï¼Œå¼€å§‹åˆå§‹åŒ–...")
    logger.info("=" * 60)
    
    try:
        from services.model_loader import llm_loader, video_loader
        
        # åŠ è½½ LLM æ¨¡å‹
        logger.info("å¼€å§‹åŠ è½½ LLM æ¨¡å‹...")
        llm_success = llm_loader.load_model()
        if llm_success:
            logger.info("âœ… LLM æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.warning("âš ï¸ LLM æ¨¡å‹åŠ è½½å¤±è´¥")
        
        # åŠ è½½è§†é¢‘æ¨¡å‹
        logger.info("å¼€å§‹åŠ è½½è§†é¢‘ç”Ÿæˆæ¨¡å‹...")
        video_success = video_loader.load_model()
        if video_success:
            logger.info("âœ… è§†é¢‘ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            logger.warning("âš ï¸ è§†é¢‘ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    logger.info("=" * 60)
    logger.info("åº”ç”¨å¯åŠ¨å®Œæˆ")
    logger.info("=" * 60)
    
    yield
    
    # å…³é—­æ—¶å¸è½½æ¨¡å‹
    logger.info("åº”ç”¨å…³é—­ï¼Œå¸è½½æ¨¡å‹...")
    try:
        from services.model_loader import llm_loader, video_loader
        llm_loader.unload_model()
        video_loader.unload_model()
    except:
        pass
```

---

## 4. ä¼˜åŒ–ç­–ç•¥

### 4.1 æ˜¾å­˜ä¼˜åŒ–

#### æ–¹æ¡ˆä¸€ï¼šFP16 åŠç²¾åº¦
```python
VIDEO_CONFIG["use_fp16"] = True  # æ˜¾å­˜å‡åŠ
```

#### æ–¹æ¡ˆäºŒï¼šå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
```python
model.enable_attention_slicing()
model.enable_xformers_memory_efficient_attention()
```

#### æ–¹æ¡ˆä¸‰ï¼šåˆ†å—å¤„ç†
```python
# åˆ†å—ç”Ÿæˆè§†é¢‘
chunk_size = 5  # æ¯æ¬¡ç”Ÿæˆ5å¸§
for i in range(0, num_frames, chunk_size):
    frames = model.generate(num_frames=chunk_size)
```

### 4.2 æ¨ç†åŠ é€Ÿ

#### æ–¹æ¡ˆä¸€ï¼šå‡å°‘æ¨ç†æ­¥æ•°
```python
VIDEO_CONFIG["num_inference_steps"] = 15  # ä»25é™ä½åˆ°15
```

#### æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ TensorRT
```bash
pip install tensorrt
```

#### æ–¹æ¡ˆä¸‰ï¼šæ‰¹é‡å¤„ç†
```python
# åŒæ—¶å¤„ç†å¤šä¸ªåœºæ™¯
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=2) as executor:
    futures = [executor.submit(generate_scene, scene) for scene in scenes]
```

### 4.3 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache

@lru_cache(maxsize=10)
def generate_video_cached(prompt: str, image_hash: str):
    """å¸¦ç¼“å­˜çš„è§†é¢‘ç”Ÿæˆ"""
    return generate_video(prompt, image)
```

---

## 5. æµ‹è¯•éªŒè¯

### 5.1 å•å…ƒæµ‹è¯•

**tests/test_video_service.py**

```python
import pytest
from services.video_service import generate_scene_video_fallback
from services.video_processor import VideoProcessor
from PIL import Image

def test_video_processor():
    """æµ‹è¯•è§†é¢‘å¤„ç†å™¨"""
    # ç”Ÿæˆå ä½ç¬¦å›¾åƒ
    image = VideoProcessor.generate_placeholder_image()
    assert image.size == (1024, 576)

def test_generate_scene_video_fallback():
    """æµ‹è¯•å¤‡ç”¨è§†é¢‘ç”Ÿæˆ"""
    scene = {
        "scene_number": 1,
        "description": "æµ‹è¯•åœºæ™¯",
        "duration": 2
    }
    video_path = generate_scene_video_fallback(scene, "test_task")
    assert os.path.exists(video_path)
```

### 5.2 é›†æˆæµ‹è¯•

```python
def test_full_video_generation():
    """æµ‹è¯•å®Œæ•´è§†é¢‘ç”Ÿæˆæµç¨‹"""
    script = {
        "title": "æµ‹è¯•è§†é¢‘",
        "scenes": [
            {"scene_number": 1, "description": "åœºæ™¯1", "duration": 2},
            {"scene_number": 2, "description": "åœºæ™¯2", "duration": 2}
        ]
    }
    
    video_path = generate_video_from_script(script, "test_task")
    assert os.path.exists(video_path)
```

### 5.3 æ€§èƒ½æµ‹è¯•

```python
import time

def test_generation_speed():
    """æµ‹è¯•ç”Ÿæˆé€Ÿåº¦"""
    start = time.time()
    video_path = generate_scene_video(scene, "test_task")
    duration = time.time() - start
    
    print(f"ç”Ÿæˆè€—æ—¶: {duration:.2f} ç§’")
    assert duration < 300  # åº”åœ¨5åˆ†é’Ÿå†…å®Œæˆ
```

---

## 6. å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: 
1. ä½¿ç”¨ FP16 åŠç²¾åº¦
2. å‡å°‘æ¨ç†æ­¥æ•°
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆModelScope T2Vï¼‰
4. å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›

### Q2: ç”Ÿæˆé€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A:
1. å‡å°‘æ¨ç†æ­¥æ•°ï¼ˆä»25åˆ°15ï¼‰
2. ä½¿ç”¨ TensorRT åŠ é€Ÿ
3. å¯ç”¨æ‰¹é‡å¤„ç†
4. ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡

### Q3: ç”Ÿæˆè´¨é‡ä¸å¥½æ€ä¹ˆåŠï¼Ÿ
A:
1. ä¼˜åŒ–æç¤ºè¯
2. å¢åŠ æ¨ç†æ­¥æ•°
3. è°ƒæ•´ guidance_scale
4. ä½¿ç”¨æ›´å¥½çš„è¾“å…¥å›¾åƒ

### Q4: å¦‚ä½•åˆ‡æ¢å…¶ä»–æ¨¡å‹ï¼Ÿ
A: ä¿®æ”¹ config.py ä¸­çš„ VIDEO_CONFIG

---

## 7. å®æ–½æ—¶é—´è¡¨

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ |
|------|------|----------|
| 1 | ä¸‹è½½æ¨¡å‹ | 1-2 å°æ—¶ |
| 2 | ä¿®æ”¹é…ç½®æ–‡ä»¶ | 10 åˆ†é’Ÿ |
| 3 | å®ç°æ¨¡å‹åŠ è½½å™¨ | 30 åˆ†é’Ÿ |
| 4 | å®ç°è§†é¢‘å¤„ç†å™¨ | 30 åˆ†é’Ÿ |
| 5 | æ›´æ–°è§†é¢‘æœåŠ¡ | 1 å°æ—¶ |
| 6 | é›†æˆåˆ°ä¸»ç¨‹åº | 20 åˆ†é’Ÿ |
| 7 | æµ‹è¯•éªŒè¯ | 1 å°æ—¶ |
| **æ€»è®¡** | | **4-5 å°æ—¶** |

---

## 8. å‚è€ƒèµ„æº

- Stable Diffusion Video: https://github.com/Stability-AI/generative-models
- Diffusers æ–‡æ¡£: https://huggingface.co/docs/diffusers
- ModelScope: https://modelscope.cn/docs
- OpenCV æ–‡æ¡£: https://docs.opencv.org/
