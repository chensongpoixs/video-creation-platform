# LLM æ¨¡å‹é›†æˆå¿«é€ŸæŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰

ç³»ç»Ÿä¼šåœ¨é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼š

```bash
cd backend
python main.py
```

æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `backend/models/chatglm3-6b/`

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨ä¸‹è½½

#### ä½¿ç”¨ä¸‹è½½è„šæœ¬

```bash
# ä» Hugging Face ä¸‹è½½
python scripts/download_model.py --source hf --model THUDM/chatglm3-6b

# ä» ModelScope ä¸‹è½½ï¼ˆå›½å†…æ¨èï¼‰
python scripts/download_model.py --source ms --model ZhipuAI/chatglm3-6b
```

#### ä½¿ç”¨ Git LFS

```bash
# å®‰è£… Git LFS
git lfs install

# å…‹éš†æ¨¡å‹ä»“åº“
cd backend/models
git clone https://huggingface.co/THUDM/chatglm3-6b

# æˆ–ä½¿ç”¨é•œåƒï¼ˆå›½å†…ï¼‰
git clone https://hf-mirror.com/THUDM/chatglm3-6b
```

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA RTX 3090/4090 (æ¨è)
- **æ˜¾å­˜**: è‡³å°‘ 12GB
- **å†…å­˜**: è‡³å°‘ 16GB RAM

### è½¯ä»¶è¦æ±‚
```bash
pip install transformers torch accelerate
```

---

## âš™ï¸ é…ç½®é€‰é¡¹

ç¼–è¾‘ `backend/config.py`:

```python
LLM_CONFIG = {
    "model_path": "./models/chatglm3-6b",  # æ¨¡å‹è·¯å¾„
    "device": "cuda",                       # cuda æˆ– cpu
    "use_fp16": True,                       # åŠç²¾åº¦ï¼ˆçœæ˜¾å­˜ï¼‰
    "use_int8": False,                      # INT8é‡åŒ–ï¼ˆæ›´çœæ˜¾å­˜ï¼‰
    "auto_download": True,                  # è‡ªåŠ¨ä¸‹è½½
}
```

### æ˜¾å­˜ä¼˜åŒ–é€‰é¡¹

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | é€Ÿåº¦ | è´¨é‡ |
|------|----------|------|------|
| FP32 | ~24GB | æ…¢ | æœ€å¥½ |
| FP16 | ~12GB | ä¸­ | å¥½ |
| INT8 | ~6GB | å¿« | è¾ƒå¥½ |

---

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæµ‹è¯•
cd backend
pytest tests/test_llm_service.py -v

# æµ‹è¯•æ¨¡å‹åŠ è½½
python -c "from services.model_loader import llm_loader; llm_loader.load_model()"
```

---

## ğŸ” éªŒè¯

å¯åŠ¨æœåŠ¡åè®¿é—®ï¼š

```bash
# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
curl http://localhost:8000/api/model/status

# æµ‹è¯•è„šæœ¬ç”Ÿæˆ
curl -X POST http://localhost:8000/api/tasks/ \
  -H "Content-Type: application/json" \
  -d '{"prompt": "åˆ¶ä½œä¸€æ®µå…³äºæ£®æ—æ¢é™©çš„çŸ­è§†é¢‘"}'
```

---

## â“ å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: è®¾ç½® `use_int8=True` æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q: ä¸‹è½½é€Ÿåº¦æ…¢ï¼Ÿ
A: ä½¿ç”¨ ModelScope æˆ–è®¾ç½®é•œåƒï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

### Q: æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ
A: æ£€æŸ¥ï¼š
1. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
2. CUDA æ˜¯å¦å¯ç”¨
3. ä¾èµ–æ˜¯å¦å®‰è£…å®Œæ•´

---

## ğŸ“š æ›´å¤šä¿¡æ¯

è¯¦ç»†æ–‡æ¡£è¯·æŸ¥çœ‹ï¼š
- [LLM é›†æˆæŒ‡å—](docs/LLM_INTEGRATION_GUIDE.md)
- [API æ–‡æ¡£](docs/API.md)
- [å¼€å‘æŒ‡å—](docs/DEVELOPMENT.md)
